{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef5828a",
   "metadata": {},
   "source": [
    "# Generate Polar Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b57359",
   "metadata": {},
   "source": [
    "This code script was created to generate the POLAR embeddings for a given pre-trained embedding model and antonym list. We created it on the basis of the original POLAR code provided here (https://github.com/Sandipan99/POLAR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523ef11",
   "metadata": {},
   "source": [
    "## 1 Data Import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de910e",
   "metadata": {},
   "source": [
    "### 1.1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d0253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import gensim\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec4194",
   "metadata": {},
   "source": [
    "### 1.2 Import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b127f82",
   "metadata": {},
   "source": [
    "Here a pre-trained word embedding model can be imported to be used as a basis for the POLAR embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d99d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only execute if model is not imported yet\n",
    "#model_glove = glove2word2vec('../data/raw/glove.twitter.27B.200d.txt','gensim_glove_twitter_200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e8e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only execute if model is not imported yet\n",
    "#model_glove = gensim.models.KeyedVectors.load_word2vec_format(\"../data/raw/reddit_word2vec.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c4f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only execute if model is not imported yet\n",
    "def generate_norm_embedding(model, output_path):\n",
    "    temp_file = open(output_path,'wb')\n",
    "    temp_file.write(str.encode(str(len(model.vocab))+' '+str(model.vector_size)+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        temp_file.write(model[each_word]/linalg.norm(model[each_word]))\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "    \n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7304f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only execute if model is not imported yet\n",
    "#generate_norm_embedding(model_glove,'reddit_word2vec.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d940e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import word embedding model\n",
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('../data/raw/reddit_word2vec.mod',binary=True)\n",
    "current_model = model_gn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1483c",
   "metadata": {},
   "source": [
    "### 1.3 Import POLAR Dimension List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a265fb6",
   "metadata": {},
   "source": [
    "In this part the list of word pairs for the POLAR dimensions is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7fc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original antonyms \n",
    "#only execute if you wna to use this list\n",
    "list_antonym = pd.read_pickle(r'../data/interim/final_antonym_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "579393ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_21191/527608963.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for each_pair in tqdm(list_new):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570f2840b87d429084f5f227f1f00f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_21191/527608963.py:38: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for each_key in tqdm(similarity_matrix):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7469a3f92543c48606d23df4a30ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_21191/527608963.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index_counter, each_key in enumerate(tqdm(all_similarity)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f98ec0d8a3c4e1095e5b7c61c6c9a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "#load business antonym list\n",
    "#only execute if you wna to use this list\n",
    "list_new= [('product', 'service'), ('essential', 'luxury'), ('technical', 'natural'), #('renewable', 'nonrenewable'),\n",
    "           ('advertising', 'secretive'), ('lease', 'sell'), ('tangible', 'intangible'), ('demand', 'supply'), #('wfh', 'wfo'),\n",
    "           ('child', 'childless'), ('remote', 'physical'), ('salary', 'goodies'), ('store', 'online'), \n",
    "           ('details', 'outlines'), ('stakeholders', 'spectators'), ('isolating', 'social'), ('goal', 'task'),\n",
    "           ('employees', 'consultant'), ('cost', 'revenue'), ('seasonal', 'temporary'), ('alliance', 'proprietorship'),\n",
    "           ('loss', 'profit'), ('integrity', 'corruption'), ('international', 'local'), ('corporate', 'individual'),\n",
    "           ('order', 'disorder'), ('solution', 'problem'), ('manager', 'worker'), ('diversity', 'uniformity'),\n",
    "           ('public', 'private'), ('strategic', 'impulsive'), ('innovator', 'follower'), ('bankruptcy', 'prosperity'),\n",
    "           ('growth', 'decline'), ('sustainable', 'unsustainable'), ('family', 'work'), ('criminal', 'rightful'),\n",
    "           ('financial', 'artisanal'), ('supplier', 'purchaser'), ('commitment', 'rejection'), ('professional', 'amateur'),\n",
    "           ('independent', 'dependent'), ('digital', 'analogue'), ('marketing', 'secret'), ('secure', 'risky'), #('longterm', 'shortterm'), \n",
    "           ('responsible', 'neglect'), ('ethical', 'unethical'), ('beneficial', 'harmful'),\n",
    "           ('diversity', 'uniformity'), ('trust', 'mistrust'), ('teamwork', 'individualism'), ('opportunity', 'threat'),\n",
    "           ('innovative', 'traditional'), ('flexible', 'rigid'), ('ambiguity', 'clarity'), ('feminine', 'masculine'),\n",
    "           ('globally', 'locally'), ('insiders', 'outsiders'), ('foreigners', 'natives'), ('minorities', 'majority'),\n",
    "           ('transparency', 'obscurity'), ('discrimination', 'impartial'), ('credible', 'deceptive'), ('environment', 'pollution'),\n",
    "           ('pressure', 'relax'), ('growth', 'decline'), ('satisfied', 'unsatisfied'), #('diplomatic', 'undiplomatic'), ('motivate', 'demotivate'), ('communicative', 'uncommunicative'), \n",
    "           ('connected', 'disconnected'), #('autonomous', 'micromanagement'), \n",
    "           ('nurture', 'neglect'), ('progressive', 'conservative'),#('rewarding', 'unrewarding'), ('bias', 'unbias'), \n",
    "           ('challenge', 'obscurity'), ('collaboration', 'silo'),\n",
    "           ('outdated', 'modern'), ('effortless', 'demanding'), ('economic', 'overpriced'), ('widespread', 'local'),\n",
    "           ('freedom', 'captive'), ('consistent', 'inconsistent')]\n",
    "\n",
    "list_new= list(dict.fromkeys(list_new).keys())\n",
    "\n",
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_new):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "\n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "final_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_list))\n",
    "\n",
    "list_antonym = final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a86fd7",
   "metadata": {},
   "source": [
    "### 1.4 Import Entities to be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a535b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import company names\n",
    "company = pd.read_csv('../data/raw/International_Fortune_GloVe.csv')\n",
    "name_list = company['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2042494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Company names for reddit embeddings\n",
    "#only execute when using reddit model\n",
    "name_list = ['walmart','homedepot','amazon','apple','cvs','toyota','volkswagen','berkshire','mckesson','samsung',\n",
    "             'ping','royal','industrial','alphabet','hon','exxon','daimler','costco','cigna','cardinal','microsoft',\n",
    "             'walgreens','allianz','kroger','jpmorgan','huawei','verizon','axa','ford','honda','general','anthem',\n",
    "             'mitsubishi','deutsche','bmw','nippon','saic','fannie','alibaba','comcast','amer','shandong','chevron',\n",
    "             'dell','bank','target','marathon','citigroup','hyundai','gazprom','facebook','royal','sony','johnson',\n",
    "             'hitachi','carrefour','bnp','bosch','tesco','aeon','hsbc','wells','general','state','intel','humana',\n",
    "             'nippon','deutsche','nissan','munich','enel','banco','procter','sk','pepsico','tencent','albertsons',\n",
    "             'basf','fedex','metlife','bank','aviation','freddie','greenland','phillips','lockheed','walt','archer',\n",
    "             'roche','xiamen','pacific','siemens','engie','legal','panasonic','reliance','brookfield','aviva','lenovo',\n",
    "             'valero','toyota','zurich','xiamen','aegon','boeing','unilever','guangzhou','prudential','airbus','mitsubishi',\n",
    "             'petrobras','hp','raytheon','softbank','prudential','tokyo','seven','alimentation','lg','goldman','industrial','aluminum',\n",
    "             'sysco','jbs','morgan','state','ptt','hca','tokio','vodafone','christian','aia','vinci','kia','eni',\n",
    "             'novartis','renault','shaanxi','cisco','korea','bayer','power','charter','merck','elo','shaanxi','zhejiang',\n",
    "             'denso','deutsche','publix','allstate','zhejiang','pemex','accenture','edeka','liberty','groupe','lloyds',\n",
    "             'tyson','bhp','woolworths','progressive','petronas','nationwide','pfizer','shandong','caterpillar','george',\n",
    "             'vale','acs','maersk','mitsubishi','ubs','oracle','energy','daiwa','jiangsu','zhejiang','dow','meiji',\n",
    "             'nike','zf','quanta','northrop','volvo','metro','usaa','chubb','banco','xiaomi','deere','barclays','cathay',\n",
    "             'mitsubishi','abbott','ck','poste','sncf','tata','fujitsu','cedar','northwestern','dollar','louis',\n",
    "             'jardine','magna','honeywell','bank','phoenix','credit','sun','thermo','repsol','tjx','shandong','travelers',\n",
    "             'capital','new','ing','tesla','cma','bharat','sap','shenzhen','coop','hyundai','anglo','mitsubishi','siemens',\n",
    "             'shanxi','jfe','haier','takeda','abb','suzuki','canon','new','samsung','kansai','enbridge','medtronic','toshiba',\n",
    "             'philip','arrow','schneider','banco','phoenix','chs','beijing','nec','zhejiang','bridgestone','guangxi',\n",
    "             'crh','xinjiang','linde','enterprise','mazda','hewlett','subaru','guangzhou','lg','kraft','guangzhou','olam',\n",
    "             'yunnan','samsung','wh','dollar','amgen','compass','coles','ericsson','banco','performance','netflix',\n",
    "             'nokia','bae','gree','gilead','eli','commonwealth','flex','rite']\n",
    "name_list = set(name_list)\n",
    "name_list = list(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d522895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_word_embedding = dict()\n",
    "for name in name_list:\n",
    "    if name in current_model.vocab:\n",
    "        name_word_embedding[name] = current_model[name]\n",
    "len(name_word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23080481",
   "metadata": {},
   "source": [
    "Other than business entities we also need generic terms embedded for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad57f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Osgoods common words\n",
    "#only execute when you want to create embeddings for these words\n",
    "common_list = ['star', 'san', 'fish', 'policeman', 'luck', 'chair', 'woman', 'love', 'trust', 'cloud', 'cup', \n",
    "               'punishment', 'doctor', 'wealth', 'hand', 'sleep', 'success', 'money', 'horse', 'knowledge',\n",
    "               'rope', 'thief', 'laughter', 'snake', 'sun', 'map', 'meat', 'bread', 'respect', 'danger', 'poison',\n",
    "               'cat', 'bird', 'lake', 'heat', 'head', 'egg', 'tongue', 'smoke', 'story', 'dog', 'fruit', 'anger', \n",
    "               'music', 'death', 'heart', 'battle', 'freedom', 'crime', 'pain', 'sympathy', 'color', 'rain', 'ear',\n",
    "               'choice', 'husband', 'wind', 'wednesday', 'river', 'need', 'hunger', 'marriage', 'hair', 'author', \n",
    "               'fire', 'power', 'moon', 'pleasure', 'water', 'tree', 'life', 'peace', 'truth', 'girl', 'tooth',\n",
    "               'guilt', 'future', 'window', 'seed', 'picture', 'stone', 'courage', 'defeat', 'hope', 'book', 'knot',\n",
    "               'food', 'purpose', 'progress', 'root', 'work', 'friend', 'noise', 'game', 'belief', 'mother', \n",
    "               'father', 'house', 'fear', 'thunder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707b5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 1500 common words\n",
    "#only execute when you want to create embeddings for these words\n",
    "with open('../data/raw/Common-eng-nouns2.txt') as f:\n",
    "    lines = f.readlines()\n",
    "lines=[line.rstrip('\\n') for line in lines]\n",
    "\n",
    "common_list=[]\n",
    "for word in lines:\n",
    "    if word in current_model.vocab:\n",
    "        common_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd633a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word_embedding = dict()\n",
    "for name in common_list:\n",
    "    common_word_embedding[name] = current_model[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a647297",
   "metadata": {},
   "source": [
    "## 2 Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bceb262",
   "metadata": {},
   "source": [
    "### 2.1 Select POLAR Dimension Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec0cd2",
   "metadata": {},
   "source": [
    "Here we select how many POLAR dimension we want to have in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e30dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 100)\n"
     ]
    }
   ],
   "source": [
    "num_antonym = 500\n",
    "\n",
    "## Find the antonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    if each_word_pair[0] in current_model.vocab:\n",
    "        if each_word_pair[1] in current_model.vocab:\n",
    "            antonymy_vector.append(current_model[each_word_pair[0]]- current_model[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855fb66",
   "metadata": {},
   "source": [
    "### 2.2 Implement Dimesion Selection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422e359",
   "metadata": {},
   "source": [
    "Now we want to specify how to select the POLAR dimensions. Therefore, we use some functions defined in the code from the original POLAR paper and adapt them to our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "572f67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "t1 = np.array(antonymy_vector)\n",
    "dimension_similarity_matrix = scipy.spatial.distance.cdist(np.array(antonymy_vector),np.array(antonymy_vector),'cosine')\n",
    "dimension_similarity_matrix = abs(1-dimension_similarity_matrix)\n",
    "\n",
    "def get_set_score(final_list, each_dim):\n",
    "    final_output = 0.0\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "\n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "\n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "\n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "            temp_score = get_set_score(final_position_index, each_dim)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e91f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_21191/2291586576.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  current_model_tensor = torch.t(torch.tensor(current_model.wv.vectors))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding size is 73\n"
     ]
    }
   ],
   "source": [
    "embedding_size = antonymy_vector.shape[0]\n",
    "print('The embedding size is', embedding_size)\n",
    "\n",
    "variance_antonymy_vector_inverse = np.linalg.pinv(np.transpose(antonymy_vector))\n",
    "variance_antonymy_vector_inverse = torch.tensor(variance_antonymy_vector_inverse)\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "current_model_tensor = torch.t(torch.tensor(current_model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "987c5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a batch approach to save work spacee for the variance calculation\n",
    "#assumes independence of batches and take average over all batches as variance\n",
    "var_list = [None for x in range(20)] # variance for each antonym in each batch\n",
    "\n",
    "for i in range(19):  # the first 19 batches, each of size 100k\n",
    "  temp = torch.matmul(variance_antonymy_vector_inverse, current_model_tensor[:,100000*i:100000*i+100000])\n",
    "  temp_var_mean = torch.var(temp, axis = 1)\n",
    "  var_list[i] = temp_var_mean.numpy()\n",
    "  del temp\n",
    "\n",
    "temp = torch.matmul(variance_antonymy_vector_inverse, current_model_tensor[:,1900000:])\n",
    "temp_var_mean = torch.var(temp, axis = 1)\n",
    "var_list[19] = temp_var_mean.numpy()\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0169b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_list = np.mean(np.array(var_list),axis = 0)\n",
    "variance_antonymy_vector = [each for each in sorted(range(len(variance_list)), key=lambda i: variance_list[i], reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a30e10",
   "metadata": {},
   "source": [
    "### 2.3 Create POLAR Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d0788",
   "metadata": {},
   "source": [
    "In this part we create the embeddings for the chosen entities and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73ae1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "    temp_dict = dict()\n",
    "\n",
    "    embedding_size = current_antonymy_vector_inverse.shape[0]   \n",
    "    print('New model size is',len(current_model), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "\n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "        temp_dict[each_word] = new_vector\n",
    "\n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "    temp_file.close()\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35cc5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    new_embedding_dict = transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)\n",
    "\n",
    "    return new_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d2ac3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_vector_method = variance_antonymy_vector\n",
    "#use the variance method for dimension selection\n",
    "antonym_500 = [list_antonym[x] for x in antonym_vector_method[:num_antonym]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eed552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the POLAR embeddings\n",
    "#change according to the entity you want to embedd\n",
    "#name_new_embedding = generate_embedding_path(name_word_embedding,'name_embeddings',True,antonym_vector_method,num_antonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeac49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_polar_dict(company_name, antonym, embedding, top_n = False, n = 10):\n",
    "  temp_dict = dict()\n",
    "  temp_polar = embedding[company_name]\n",
    "\n",
    "  if top_n:\n",
    "    idx = np.argsort([abs(x) for x in temp_polar])[-n:]\n",
    "    for i in idx:\n",
    "      print(antonym[i],temp_polar[i],'\\n')\n",
    "\n",
    "\n",
    "  if len(antonym) == len(temp_polar):\n",
    "    for a in range(len(antonym)):\n",
    "      temp_dict[antonym[a]] = temp_polar[a]\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f27866",
   "metadata": {},
   "source": [
    "## 3 Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693f12c",
   "metadata": {},
   "source": [
    "Here we save the POLAR model generated for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b657075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create csv file for the embedding matrix\n",
    "#df = dict()\n",
    "#for t in name_list:\n",
    "    #if t in current_model.vocab:\n",
    "        #df[t] = make_polar_dict(t, antonym_500, name_new_embedding)\n",
    "\n",
    "#new_df = pd.DataFrame(df).transpose()\n",
    "\n",
    "# change columns to better read names\n",
    "#new_columns = []\n",
    "\n",
    "#for pair in antonym_500:\n",
    "  #temp = pair[0]+''+pair[1]\n",
    "  #new_columns.append(temp)\n",
    "\n",
    "#new_df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab080fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the file locally\n",
    "#new_df.to_csv('POLAR-Reddit-org-antonyms-inter.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}